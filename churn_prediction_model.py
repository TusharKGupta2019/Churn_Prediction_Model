# -*- coding: utf-8 -*-
"""Churn Prediction Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16y77sasFiKp2BVQYafdSBCjycn6lr2DM
"""

# Importing the Libararies

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score

df = pd.read_csv('Churn_Modelling.csv')

df.head()

df.info()

# Checking Null Values

df.isnull().sum()

# Checking Duplicates

df.duplicated().sum()

# Converting Categorical data into numeric

label_encoder = LabelEncoder()
df['Gender']= label_encoder.fit_transform(df['Gender'])
df = pd.get_dummies(df, columns=['Geography'], drop_first= True)

# Checking the head again for feature selection as per logical understanding of data
# Gender has been changed to 0 & 1
# Geography coulmn has changed too

df.head()

# Feature Selection

features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Gender', 'Geography_Spain','Geography_Germany']

X = df[features]
y = df['Exited']

# Spliting data into testing and training

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Scaling

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Checking Scaled Values

X_train[:5], X_test[:5]

# Randomn Forest

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predicting

y_pred = model.predict(X_test)

# Confusion Matrix, Accuracy and Classification Report

conf_matrix = confusion_matrix(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

print("Confusion Matrix:")
print(conf_matrix)
print("\nAccuracy:", accuracy)
print("\nClassification Report:")
print(classification_rep)

# Feature Importance

importances = model.feature_importances_
indices = np.argsort(importances)[::-1]
names = [features[i] for i in indices]

plt.figure(figsize=(10, 6))
plt.title("Feature Importance")
plt.barh(range(X.shape[1]), importances[indices])
plt.yticks(range(X.shape[1]), names)
plt.show

# Now lets apply Logistic Regression to compare

from sklearn.linear_model import LogisticRegression

# Build and Train the Logistic Regression Model
log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train, y_train)

# Make Predictions
y_pred_log_reg = log_reg.predict(X_test)

# Evaluate the model
conf_matrix_log_reg = confusion_matrix(y_test, y_pred_log_reg)
accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)
classification_rep_log_reg = classification_report(y_test, y_pred_log_reg)

print("Logistic Regression - Confusion Matrix:")
print(conf_matrix_log_reg)
print("\nLogistic Regression - Accuracy:", accuracy_log_reg)
print("\nLogistic Regression - Classification Report:")
print(classification_rep_log_reg)

# Now lets apply SVM to compare

from sklearn.svm import SVC

# Build and Train the SVM Model
svm_model = SVC(kernel ='linear' ,random_state=42)
svm_model.fit(X_train, y_train)

# Make Predictions
y_pred_svm = svm_model.predict(X_test)

# Evaluate the SVM Model
accuracy_svm = accuracy_score(y_test, y_pred_svm)
classification_rep_svm = classification_report(y_test, y_pred_svm)
confusion_matrix= confusion_matrix(y_test, y_pred_svm)

print("SVM - Confusion Matrix:")
print(confusion_matrix)
print("\nSVM - Accuracy:", accuracy_svm)
print("\nSVM - Classification Report:")
print(classification_rep_svm)

# As per the warnings this model was not able to predict properly as for some lables it did not predict any samples.

# Lets try KNN Model Now

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix

# Build a model
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)

# Make Predictions
y_pred_knn = knn_model.predict(X_test)

# Evaluate the model
accuracy_knn = accuracy_score(y_test, y_pred_knn)
class_rep_knn = classification_report(y_test, y_pred_knn)
conf_matrix_knn= confusion_matrix(y_test, y_pred_knn)

print("KNN - Confusion Matrix:")
print(conf_matrix_knn)
print("\nKNN - Accuracy:", accuracy_knn)
print("\nKNN - Classification Report:")
print(class_rep_knn)

# Lets apply Gradient Boosting Classifier now

from sklearn.ensemble import GradientBoostingClassifier

# Build and Train the Gradient Boosting Classifier
gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)
gb_model.fit(X_train, y_train)

# Make Predictions
y_pred_gb = gb_model.predict(X_test)

# Evaluate the model
accuracy_gb = accuracy_score(y_test, y_pred_gb)
classification_rep_gb = classification_report(y_test, y_pred_gb)
confusion_matrix= confusion_matrix(y_test, y_pred_gb)

print("Gradient Boosting - Confusion Matrix:")
print(confusion_matrix)
print("\nGradient Boosting - Accuracy:", accuracy_gb)
print("\nGradient Boosting - Classification Report:")
print(classification_rep_gb)

# Feature Engineering

df = pd.read_csv('Churn_Modelling.csv')

# Binary feature for balance
df['Zero_Balance']=(df['Balance']==0).astype(int)

# Age Groups

df['Age_Group'] = pd.cut(df['Age'], bins=[18, 25, 35, 45, 55, 65, 75, 85, 95], labels=['18-25', '26-35', '36-45', '46-55', '56-65', '66-75', '76-85', '86-95'])

# Balance to Salary Ratio

df['BSRatio']= df['Balance']/df['EstimatedSalary']

# Interaction Feature between Numofproducts and Isactivmember

df['ProductUsage']= df['NumOfProducts']*df['IsActiveMember']

# Tenure Grouping

df['Tenure_Group']=pd.cut(df['Tenure'], bins=[0, 2,5,7,10], labels=['0-2', '3-5', '6-7', '8-10'])

label_encoder = LabelEncoder()
df['Gender']= label_encoder.fit_transform(df['Gender'])
df = pd.get_dummies(df, columns=['Geography'], drop_first= True)
df['Male_Germany']= df['Gender']*df['Geography_Germany']
df['Male_Spain']= df['Gender']*df['Geography_Spain']

df = pd.get_dummies(df, columns=['Age_Group','Tenure_Group'], drop_first= True)

features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Gender', 'Geography_Spain','Geography_Germany','Zero_Balance','BSRatio','ProductUsage','Male_Germany','Male_Spain'] + [col for col in df.columns if 'Age_Group_' in col or 'Tenure_Group_' in col]

X = df[features]
y = df['Exited']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

from sklearn.metrics import confusion_matrix

cnf_matrix = confusion_matrix(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

print("Confusion Matrix:")
print(cnf_matrix)
print("\nAccuracy:", accuracy)
print("\nClassification Report:")
print(classification_rep)

# Not much improvement after feature engineering, However, model is performing consistently.